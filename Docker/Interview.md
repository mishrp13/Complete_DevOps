High-level overview (start with this)

In our organization at CitiBank, we use LightSpeed as an enterprise CI/CD orchestration platform. It does not replace tools like GitHub, Tekton, Harness, or OpenShift; instead, it coordinates and governs them to provide standardized, secure, and compliant software delivery across teams.

What LightSpeed does (core responsibility)

You can explain LightSpeed as:

LightSpeed acts as the control plane for CI/CD. It standardizes pipelines, enforces Citi security and compliance policies, manages approvals, and orchestrates the execution of pipelines across multiple tools.

Key responsibilities:

Pipeline orchestration and governance

Enterprise security and compliance enforcement

Standardized CI/CD workflows across teams

Integration with best-of-breed tools (GitHub, Tekton, Harness, OpenShift)

Tool-by-tool integration explanation
1. GitHub ‚Äì Source Control & Trigger

GitHub is used as the source code repository. Developers push code or raise pull requests in GitHub.

How it integrates:

GitHub webhooks notify LightSpeed when:

Code is pushed

A PR is created or merged

LightSpeed validates:

Branching strategy

PR approvals

Required checks (security scans, code quality)

‚û°Ô∏è GitHub = Code + Triggers

2. Tekton ‚Äì CI Pipeline Execution

Tekton is used for CI execution ‚Äî building, testing, and scanning the application.

How it works:

LightSpeed triggers Tekton pipelines

Tekton performs:

Code checkout

Build (e.g., Maven/Gradle/npm)

Unit tests

Static code analysis

Image build

Artifacts and container images are generated

‚û°Ô∏è Tekton = Build & Test Engine

3. Harness ‚Äì CD & Deployment Strategy

Harness is used for continuous delivery, managing how applications are deployed.

Harness handles:

Deployment strategies:

Blue-Green

Canary

Rolling updates

Environment promotions:

Dev ‚Üí QA ‚Üí UAT ‚Üí Prod

Approval workflows (manual or automated)

Rollbacks in case of failure

LightSpeed:

Orchestrates when Harness is invoked

Ensures compliance gates are passed before deployment

‚û°Ô∏è Harness = Controlled & Safe Deployment

4. OpenShift ‚Äì Runtime Platform

OpenShift is our Kubernetes-based platform where applications actually run.

OpenShift provides:

Container orchestration

Scaling and self-healing

Security and network policies

Namespace isolation per environment

Harness deploys applications into OpenShift clusters.

‚û°Ô∏è OpenShift = Execution & Runtime Environment

End-to-end CI/CD flow (very important for interviews)

You can say this almost verbatim:

A developer pushes code to GitHub

GitHub triggers LightSpeed

LightSpeed validates governance and starts the pipeline

Tekton runs CI tasks (build, test, scan, image creation)

On success, LightSpeed triggers Harness

Harness deploys the application to OpenShift

OpenShift runs and manages the application in the target environment

Why Citi uses LightSpeed (business angle)

Interviewers love this part.

Citi uses LightSpeed to achieve enterprise-level consistency, security, and compliance across thousands of applications and teams.

Benefits:

Standard CI/CD across the bank

Built-in security and regulatory compliance

Reduced manual errors

Faster, safer releases

Auditability and traceability

One-line summary (great closing statement)

LightSpeed is the CI/CD orchestrator that governs and coordinates GitHub for source control, Tekton for CI, Harness for CD, and OpenShift for runtime deployment in a secure, enterprise-compliant manner.

******
My name is Prabal Mishra, and I am a DevOps Engineer with experience in building scalable and automated infrastructure on AWS Cloud, with strong expertise in Kubernetes,Docker, and CI/CD pipelines.

In my current role at Tata Consultancy Services, I have implemented containerization and Kubernetes-based orchestration that significantly reduced environment-related issues, designed CI/CD pipelines with Jenkins to accelerate deployments, and ensured high availability for production environments. 

I hold a B.E. in Electronics and Communication Engineering from Chandigarh University, and I am passionate about automating infrastructure, improving release velocity, and ensuring reliable cloud-native systems.

I‚Äôm excited about this opportunity because I want to bring my expertise in DevOps practices, cloud automation, and production reliability to contribute to your team‚Äôs success.‚Äù

***
Q1. u have done HPA but only one pod is running how will you troubleshoot?


Step-by-step troubleshooting:

1Ô∏è‚É£ Check HPA status

kubectl get hpa
kubectl describe hpa <hpa-name>


Check target vs current CPU/memory

2Ô∏è‚É£ Verify metrics server

kubectl get pods -n kube-system | grep metrics


If metrics server is not running ‚Üí HPA won‚Äôt scale

3Ô∏è‚É£ Check pod resource requests

kubectl describe pod <pod-name>


üëâ HPA requires CPU/memory requests to be set

4Ô∏è‚É£ Check load

kubectl top pods


If actual usage is below threshold ‚Üí no scaling

5Ô∏è‚É£ Check maxReplicas

kubectl get hpa <hpa-name> -o yaml


Ensure maxReplicas > 1

Interview-ready answer:

‚ÄúI check HPA metrics, metrics server, resource requests, and actual load. Most commonly it‚Äôs missing CPU requests or metrics server issues.‚Äù

Q2. ur pod is crashloop back state how will you troubleshoot?

1Ô∏è‚É£ Check pod status

kubectl describe pod <pod-name>


2Ô∏è‚É£ Check logs

kubectl logs <pod-name>
kubectl logs <pod-name> --previous


3Ô∏è‚É£ Check container command & args

kubectl get pod <pod-name> -o yaml


4Ô∏è‚É£ Check config & secrets

Missing ENV variables

Wrong config map

Secret not mounted

5Ô∏è‚É£ Check resource limits

OOMKilled?

kubectl describe pod | grep -i oom

Interview-ready answer:

‚ÄúCrashLoopBackOff usually means app startup failure, wrong configs, missing secrets, or OOM issues. Logs are the first thing I check.‚Äù

Q3. ur pod is in pending state how will you troubleshoot?
1Ô∏è‚É£ Describe the pod

kubectl describe pod <pod-name>


2Ô∏è‚É£ Check node availability

kubectl get nodes


3Ô∏è‚É£ Check resource requests

CPU/memory too high?

No node can schedule it

4Ô∏è‚É£ Check PVC

kubectl get pvc


Pending PVC ‚Üí pod stays pending

5Ô∏è‚É£ Check taints & tolerations

kubectl describe node <node-name>

Interview-ready answer:

‚ÄúPending state is mostly scheduling issues‚Äîinsufficient resources, PVC pending, or taints without tolerations.‚Äù

Q4. ur build is success and showing green but deployment is failing?
1Ô∏è‚É£ Check deployment events

kubectl describe deployment <deployment-name>


2Ô∏è‚É£ Check pod status

kubectl get pods


3Ô∏è‚É£ Check image pull

kubectl describe pod <pod>


ImagePullBackOff?

Wrong image tag?

4Ô∏è‚É£ Check environment

ConfigMaps

Secrets

Wrong environment variables

5Ô∏è‚É£ Check readiness probes

Pod running but not ready

Interview-ready answer:

‚ÄúBuild success only means image creation. Deployment can fail due to wrong image tags, missing configs, secrets, or probe failures.‚Äù
Q5. if your etcd is down does it impact?

Short answer:

üëâ YES ‚Äì major impact

What happens:

Kubernetes state is stored in etcd

API server cannot read/write cluster state

No new pods, deployments, or scaling

Running pods continue but no changes possible

Interview-ready answer:

‚ÄúIf etcd is down, the control plane is effectively read-only. Existing workloads run, but cluster changes fail.‚Äù

Q6. where u will define the disk of the node in k8s cluster?
üëâ Kubernetes does NOT manage node disks directly

Disk is defined at:

Bare-metal ‚Üí OS level (LVM, mount points)

VMware ‚Üí VM disk configuration

Cloud/OpenStack ‚Üí volume attached to VM

In Kubernetes you define:

PersistentVolume (PV)

PersistentVolumeClaim (PVC)

apiVersion: v1
kind: PersistentVolume
spec:
  capacity:
    storage: 10Gi

Interview-ready answer:

‚ÄúNode disks are configured at infrastructure/OS level. Kubernetes consumes storage using PVs and PVCs, not by defining node disks.‚Äù

üß† One-Line Killer Answers (Memorize)
Scenario	One-liner
HPA not scaling	Metrics or resource requests missing
CrashLoopBackOff	App startup/config/resource issue
Pod Pending	Scheduling or storage problem
Build green, deploy fail	Runtime config/image/probe issue
etcd down	Control plane frozen
Node disk	Infra-level, not Kubernetes
